{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c86c84-8262-47af-85eb-acf49e375669",
   "metadata": {},
   "source": [
    "## Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a1e22-416b-4406-8f55-f0c254e1a833",
   "metadata": {},
   "source": [
    "* List one advantage that regular expressions have over building an ML pattern-matcher, and one disadvantage.\n",
    "\n",
    "Ans: \n",
    "Regex is powerful, it is able to match different groups with a pattern, the expression is simple and efficient, and can be interpretable. For example, the telephone number in Canada or United States.\n",
    "\n",
    "However, this pattern has to be constructed according to a certain rule, this indicates that Regex will not generalize well on the real-world dataset. For example, the telephone number in China will not start with +1 or 10 digit phone number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7c4b1-1169-468b-8779-c6e3b3fc5a7c",
   "metadata": {},
   "source": [
    "* There are two ways of discovering a pattern against the start of a string.  Describe them.\n",
    "\n",
    "Ans:\n",
    "We can use regex `r'^something'`, this allows us to find a str that start with 'something'. Another way is to use re.match('something', text), this also allows us to find the same group as the regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9ad3d-80e1-4eaa-be03-aba7108c523a",
   "metadata": {},
   "source": [
    "* How would you detect tokens that contain non-English characters (assume that digits and punctuation like .?!;, are all ``English'')?  Only use methods described in class.\n",
    "\n",
    "Ans:\n",
    "Assume we have an already tokenized list of words that allow us to iterate, then use a for loop to iterate over every word in the list with a regex pattern `[^a-zA-Z.?!;]` to see if there is any matching pattern, if there is a return, the the token contains non-English characters, if return is None, then it means not. However, we did not consider '-' in the pattern, so the words with a hyphen or hyphens in it will also be considered as non-English word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03fffe-dcf5-4c14-ac4d-e71712b4349c",
   "metadata": {},
   "source": [
    "* Describe the purpose of the various types of brackets in regexes, and how they differ.\n",
    "\n",
    "Ans: `()` Creates a capture group, for example, (abc) means I want to capture a group that contains abc in it; `[]` Matches any one of the character in the [], for example: `[a-z]` means we want to capture the character in lower case a to lower case z; `{}` is used to restrict the number of previous character or group, and {} can be used in various way, such as a{1} means we only want to match one for the previous character/group a, a{1,2} means we can match 1 or 2 of previous character/group, and a{2,} means match two or more of previous character/group a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b83d9-1960-473a-98fd-fc9f4996d015",
   "metadata": {},
   "source": [
    "* Could you build a regular expression that detects regular expressions?  If so, what kind of patterns would you look for?  If not, what difficulties are there?\n",
    "\n",
    "Ans: We can build a regex to detect if the text contains something that looks like a regex, but we can not have a 100% regex for this task. For similar search, we can try to search if the text contains anything that has a `\\`, `*`, `^`, `()`, `[]`, `[(?:)|(?=)|(?<=)]`, or anything that a regex may have. However, this will result lots of false positives since not only regex uses these expressions, math, code, or nearly everywhere uses these expressions too, so it is impossible to have a 100% perfect regex to capture regular expression. Furthermore, we also don't know what Engine they are using, a example is that for a regular expression appear in the text, it will have `\\` escape, however, we don't need that in python re, so it will be hard to tell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37171450-96ce-4741-80dc-656d2baa6395",
   "metadata": {},
   "source": [
    "* Do you think that regexes could be used to discover palindromes (ie, words that are written the same backwards and forwards, like ``racecar'').  What about palindromic sentences, where the words are identical, if not the characters (like ``Wisdom shapes destiny as destiny shapes wisdom.'')?  If so, what features of regexes would you need to use?  If not, explain why not.\n",
    "\n",
    "Ans: It is possible, we can do if else statements, but it will be a very long regex pattern. Also, according to chatgpt, there is a function called backreference in some regex engine, we can also use that to match the pattern. However, both methods are restricted by the length, we need to build a pattern that matches exact length, for example, 5 character palindrome. If we want to match all palindromes with any length, I don't think we can do it unless there is a function that we can utlize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45078cf-e372-4e66-ba8f-ee0ce389b083",
   "metadata": {},
   "source": [
    "* Imagine we have a spell-checker that can identify common misspellings of words by replacing certain letters with a capture group that contains letters that are nearby on the keyboard.  How aggressive of a regex would we want to write for this (ie, how many letters in the word would we want to replace with a group)?  Explain.  Are there any types of typos that regexes couldn't handle?\n",
    "\n",
    "Ans: We need to balance the recall and precision, hence instead of restrict numer of letters replaces, we can set different number of replaces for strings with different length. For example, 'check' is more likely to have one typo instead of 2, so we can restrict the number of replacement into 1, 'responsibility' is a longer string and will tend to have more typos than 'check', hence we can increase the number of replacement. By doing so, we can balance the Precision and recall for our matches. For the typos that regex cannot handle, I can think of switching orders between two letters, for exmaple: 'check' vs. 'cehck', this is because each word is different, and we need to build specific pattern if we want to address this kind of typo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22765c7-0b79-4f19-9248-94b6ecbaaf0a",
   "metadata": {},
   "source": [
    "* In class, we've taken a brief look at both prefixes and suffixes, but there are other ways of inflecting words.  ``circumfixes`` wrap around a word, such as the German past participle marker ``ge-t`` (``ich spiele`` - ``I play``; ``ich habe gespielt`` - I have played).  Likewise, ``infixes`` occur inside of a word - ``cupful`` + Plural -> ``cupsful``, or in Tagalog: ``bili`` -> ``to buy``; ``bumili`` -> ``X is buying``.  Finally, ``reduplication`` occurs when part or all of a token is repeated to indicate some feature, such as repetition or future intent in Tagalog: ``aray`` -> ``day``; ``arayaray`` -> everyday; ``basa`` -> ``to read``; ``babasa`` -> ``will read (in the future)``.  Which of these are best suited for regexes, and which features of regexes are they exploiting?  Are there any that are mostly unsuited to regexes?  Why?\n",
    "\n",
    "Ans: In the first place, circumfixes best suited for regexes since the circumfixes is the combination of prefix and suffix, which regex handles both thing very well, using `r'^prefix(.+)suffix$'` can match with the circumfixes we want. Secondly, infixes are also searchable, but will result lots of false positives since the location of infixes are not fixed, which other normal words may also use the same pattern within the vocabulary, for example, for 'bumili', we can use `r'\\w+(um){1}\\w+'` to match. Furthermore for reduplication, regex will not be a good idead since it has no exact pattern of how the word will reduplicate itself to form a new meaning, regex is just a search tool instead of a language model so it will not be able to understand the meaning of the word neither the semantics of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a49d69-c94d-4ae8-bdce-221a8b834eb6",
   "metadata": {},
   "source": [
    "## Letrue 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a6752-4679-47b2-ad8a-e6c8c17e3d12",
   "metadata": {},
   "source": [
    "* Why is XML well-suited to representing linguistic data?\n",
    "\n",
    "Ans: This is because XML has a clear structure where each level can be extracted, annotated, or labeled, such as POS, semantics, or any information that is helpful, while keeping the original text unchanged or uninfluenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf9355-9359-4bb1-9a3d-15f3344b409c",
   "metadata": {},
   "source": [
    "* How would we find all images in an HTML document?\n",
    "Ans: We can write a for loop to find all `<img>` node, then access their src to obtain the images.\n",
    "`[i['src'] for i in soup.find_all('img')]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dde41f-8564-4133-919c-2bba15b7b2dc",
   "metadata": {},
   "source": [
    "* What kinds of tags might be useful in the following text (describe at least two): ``But you liked Rashomon!`` ``That's not how I remember it!``\n",
    "\n",
    "Ans: We can add speakers' information to the text, since the example is clearly recording two different person talking. Secondly, we can add `title` in it, since 'Rashomon' is a movie title, which is a proper noun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d38c8-067f-44a4-836a-2335e992a215",
   "metadata": {},
   "source": [
    "* XML can be opened by most plain-text text editors.  Name a benefit and a disadvantage of this feature.\n",
    "\n",
    "Ans: The benefit is that nearly everybody can open the XML file without doing any extra effort, such as setting new environment just for open a XML and edit it.\n",
    "The disadvantage is that XML relies on a specific structure, using plain-text text editor may increase the probabilities of wrongly editing the structure of XML, which will evetually destroy the whole XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f408d3-1c7d-4262-9b76-e80e9f7b63c5",
   "metadata": {},
   "source": [
    "* Beautiful Soup parses the children of a tag as a list. Why do you think they didn't use a set, instead, given the faster access times?  Give 2 reasons, and briefly explain.\n",
    "\n",
    "Ans: In the first place, set does not preserve order, turning children as set will lose the order of the labels, we can no longer reverse the set in to ordered list any more. Secondly, there may have multiple same labels in a child node, turning them into set will lose information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb6aed-a7f8-48b8-b1e1-3c4ebcc93a38",
   "metadata": {},
   "source": [
    "* Suppose you've trained a Named Entity Recognition (NER) model using XML-annotated text data, but it consistently fails to recognize locations. What steps would you take to determine if the problem lies with the model, the training data, or both? What resources would you need to investigate further?\n",
    "\n",
    "Ans: The first step is to check the training data, it is possible that the location is not properly extracted, or there are different types of a same location labeled in the training set, such as NewYork, newyork, Newyork, new york, I can check the location through turning the locations into a set to mannually check if the locations are properly labeled. Moreover, checking the class distribution of the location also matters, it is possible that each location only have 1-2 observations which the model will not learn any thing from them except for the noises. If the training set works fine, then it will be the issue of the model or the pipeline. Also, we can check the performance of the model to identify the reason, by checking the training score, validation score, and the test score, we will be able to know the cause, if the training score is 0, then it will be the issue of training set itself since the model does not learn any thing from the training set, however, if the score on training set is fine but perform poorly on validation set or test set, then it might be possible that we have a class imbalance in the training set or we need to tune the hyperparameter to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bb9da-57e0-4203-b8db-301893fc9f1b",
   "metadata": {},
   "source": [
    "* A colleague reorganizes the hierarchy and ordering of elements in a shared XML file to ``make it cleaner'', but another team reports that their software no longer interprets the file correctly. Identify two ways that you would approach locating and fixing the problem.\n",
    "\n",
    "Ans: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fe278-55c8-44f1-8995-e82489fff596",
   "metadata": {},
   "source": [
    "* You've been hired by a company that is working with their own version of XML that they call ``NQAXML'' (Not-Quite-As-eXtensible Markup Language).  It provides stronger restrictions on tag names (they must be all uppercase, and no longer than 10 characters long), and it doesn't allow nested spans with identically-named tags.  Like HTML, it also has a set of tags that must appear in every document.  Furthermore, since the company deals with multilingual data, every tag must have the attribute: ``lang'', describing the language of the span.  Describe your process for creating a data validator that takes an XML file, and ensures that it satisties the rules of NQAXML.\n",
    "\n",
    "Ans: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80d945-c60b-47a8-b8a2-526c13b69ba4",
   "metadata": {},
   "source": [
    "## Lecture 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4d84-243c-4a3b-a702-78a1041d2cff",
   "metadata": {},
   "source": [
    "* Can you think of any classes of words in English where the stem and the lemma will always be identical? Why is that of little interest to us?\n",
    "\n",
    "Ans: Most of the proper nouns, prepositions, conjunction, articles, and modal verbs have identical stem and lemma. We don't have interest on them because most of the words are stop words, meanwhile these words have no variations which means we can not examine the variability of the model/pattern on these words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a46d73-e711-4f8b-b2bf-ce3581870d9c",
   "metadata": {},
   "source": [
    "* What might the training data for a sentence segmenter look like?  Do you think it would be easy or hard to train?  Explain briefly.\n",
    "\n",
    "Ans: I believe the training data will have a label indicating where the sentence end, with that model can capture the pattern of ending a sentence in a language. For difficulties, I believe that it depends on the languages, for example, Chinese is relatively easier to train because Chinese sentence normally ends with a clear sign such as period, exclamation mark, or a stop-meaning punctuation, however, for English, we may include abbrevatives such as U.S., P.M., which is also period but has different meaning. Hence, the difficulty of training a sentence segmenter depends on the language if we have a same qualities for labeling in both languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e21a26-5a56-4cbd-93c6-f2b5a9327548",
   "metadata": {},
   "source": [
    "* What impact does lemmatization or stemming have with respect to the Zipfian curve?  How might that affect our algorithms?\n",
    "\n",
    "Ans: Zipfian curve indicates the frequencies of the words, those words with high frequencies appears at the head of the Zip's curve while low frequency words appears at the tail of the Zip's curve. With lemmatization or stemming, the Zip's curve will have a higher head meanwhile a shorter tail because those complex words are more likely to be lemmatized or stem into a shorter pattern: run = run/runs/running/ran. The most significant affect is that we will have a more dense distribution instead of having words with frequencies = 1 or 2 at the tail, which also means that we will have a smaller model if we train it with a lemmatized or stem corpora. However, the disadvantage is that we will lose most of the syntax information of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b4642-42a6-4f90-8ec5-19858283774a",
   "metadata": {},
   "source": [
    "* If you were to encounter an alien text, which encoding might you want to use to digitize it?  Explain briefly.\n",
    "\n",
    "Ans: Unicode will the suitable encoding method to digitize an unseen alien text/language, it is because Unicode has more expressibility compare to ASCII or Latin-1, it does not rely on a alphabetical form, meanwhile, Unicode can expand or update the new seen text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5e5d9-9c68-4e9c-95a8-56ecafd7ae9e",
   "metadata": {},
   "source": [
    "* What are two advantages of using .py files over .ipynb files for deployment, and two reasons why .ipynb files are preferred for prototyping or development?\n",
    "\n",
    "Ans: .py file has less overhead compare with ipynb since ipynb include format/code chunk information, where .py file is just a script with code. .py files are also easy to use since we can directly import different .py script as a package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186fb5b-568d-4cb5-9063-fad1944d7b14",
   "metadata": {},
   "source": [
    "* Do you think that we could do lemmatization before machine translation?  Provide 1 argument for why it might help, and one for why it might make things more complicated.  List any assumptions.\n",
    "\n",
    "Ans: It depends on the task of the model, if the model aims to translate a single word, lemmatization will be helpful since it decrease the density of the language data, which trims lots of noises from the data. However, if the task is to do a full sentence translation, lemmatization will be harmful since it also trims out critical syntax information of the word, if the model want to translate, it has to also predict the syntax of the word, which increase the difficulty of predicting the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2196c95-e8fd-4660-a714-f8931781da7c",
   "metadata": {},
   "source": [
    "* Imagine that you're working with a linguist who is not very good with technology.  They store all of their data in .docx files, scattered across their desktop.  What arguments would you make for them to convert to .tsv or .json, and how would you alleviate their worries that they wouldn't be able to access or modify their information (no, you can't teach them Python)?\n",
    "\n",
    "Ans: .docx is a unstructured file type that has varies hidden attributes such as font size, annotations, fonts, etc, which is not helpful for data analysis at all and might break the structure of the data. On contrary, tsv or json are more reliable, they are all structured, and ready for data analysis. .tsv can be open with Excel, Office, or R, nearly all table softwares are compatiable with .tsv; .json file can be open with plain-text text editor, where the structure is also clear, we can know the parents or children at the first glance. Furthermore, .tsv or .json is beneficial for search, statistical analysis or collaboration with other researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9a7b7-5658-4c11-8090-6ce954b087b4",
   "metadata": {},
   "source": [
    "* Imagine that you find an (assumedly) important file buried on a hard drive found in the basement of a university.  You are trying to access the data, but realize it is corrupted.  Some of the bits have been flipped (switched from 0 to 1, or 1 to 0), and others have been completely deleted.  You don't know the encoding, and you don't know the language the data is written in.  Chardet can't help you.  What are some tests you could run to try to establish and restore at least some of the data? (Hint: remember that a ``byte'' is 8-bits, and that UTF-8 must be at least 1 byte, or 8 bits, UTF-16 is at least 2 bytes, or 16 bits, and UTF-32 is always 4 bytes, or 32 bits).\n",
    "\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a6cb5-dd3c-4142-bb71-a7fca037ed47",
   "metadata": {},
   "source": [
    "## Lecture 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dce434-47cc-4a35-a780-f3b17fed3704",
   "metadata": {},
   "source": [
    "* Briefly describe the role of ``recurrence``, and when we need it for linguistic data.\n",
    "\n",
    "Ans: Recurrence means that we take the last hidden layer's output as input for the next timestep, we need recurrence when we need the context information of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22ea21-b09b-43e4-970d-003c07b2fdba",
   "metadata": {},
   "source": [
    "* Why is stackability so important in neural networks?\n",
    "\n",
    "Ans: It allows the neural network to learn layer by layer, stackability allows the model to learn from the most basic pattern such as letter, then pass the results to next layer so that now model can interpret word, then repeat this step multiple times so that the model can now understand syntax meaning, POS tag, or any complex pattern that a regular model won't understand in a single train process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3252625-0f5e-47b8-9374-bcc1b21c36f5",
   "metadata": {},
   "source": [
    "* Describe why ``positional embeddings`` allowed for transformers to increase performance significantly over previous models.\n",
    "\n",
    "Ans: The issue is that transformers use GPU parallel computations instead of CPU recurrence computations. Positional embeddings, add position indicators into the model so that transformer now can know the position of the word as well as the vector and distance of the word, this helps the transformer to understand the grammar meaning in a sentence. Furthermore, since transformer uses paralell computations, it is a lot faster than previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a1445-e990-4f72-ad19-8e6f74f70867",
   "metadata": {},
   "source": [
    "* What is attention, and why is it important when processing linguistic data?  Give a linguistic example where it would be useful (not an example from class).\n",
    "\n",
    "Ans: Attention is an algorithm that allows the transformer model to dynamically asign weights for different location in a text instead of treat every word equally important. This allows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d14de-3587-4924-a8fe-ffe53314507a",
   "metadata": {},
   "source": [
    "* What do encoders ``encode'', and how does it help linguistic processing?\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b9a59-855e-4043-80c0-b2b401edb3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:block1_env]",
   "language": "python",
   "name": "conda-env-block1_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
